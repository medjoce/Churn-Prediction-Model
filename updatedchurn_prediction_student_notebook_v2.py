# -*- coding: utf-8 -*-
"""UpdatedChurn Prediction_Student_Notebook_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OEXyjelKFU3cc_DfXYOVJbKqsMY4nvsk

___

<p style="text-align: center;"><img src="https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV" class="img-fluid" alt="CLRSWY"></p>

___

# WELCOME!

Welcome to "***Employee Churn Analysis Project***". This is the second project of Capstone Project Series, which you will be able to build your own classification models for a variety of business settings. 

Also you will research what is Employee Churn?, How it is different from customer churn, Exploratory data analysis and visualization of employee churn dataset using ***matplotlib*** and ***seaborn***, model building and evaluation using python ***scikit-learn*** and ***Tensorflow-Keras*** packages. 

You will be able to implement classification techniques in Python. Using Scikit-Learn allowing you to successfully make predictions with Distance Based, Bagging, Boosting algorithms for this project. On the other hand, for Deep Learning you will use Tensorflow-Keras. 

At the end of the project, you will have the opportunity to deploy your model using *Streamlit*.

Before diving into the project, please take a look at the determines and project structure.

- NOTE: This project assumes that you already know the basics of coding in Python and are familiar with model deployement as well as the theory behind Distance Based, Bagging, Boosting algorithms, and Confusion Matrices. You can try more models and methods beside these to improve your model metrics.

# #Determines
In this project you have HR data of a company. A study is requested from you to predict which employee will churn by using this data.

The HR dataset has 14,999 samples. In the given dataset, you have two types of employee one who stayed and another who left the company.

You can describe 10 attributes in detail as:
- ***satisfaction_level:*** It is employee satisfaction point, which ranges from 0-1.
- ***last_evaluation:*** It is evaluated performance by the employer, which also ranges from 0-1.
- ***number_projects:*** How many of projects assigned to an employee?
- ***average_monthly_hours:*** How many hours in averega an employee worked in a month?
- **time_spent_company:** time_spent_company means employee experience. The number of years spent by an employee in the company.
- ***work_accident:*** Whether an employee has had a work accident or not.
- ***promotion_last_5years:*** Whether an employee has had a promotion in the last 5 years or not.
- ***Departments:*** Employee's working department/division.
- ***Salary:*** Salary level of the employee such as low, medium and high.
- ***left:*** Whether the employee has left the company or not.

First of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, you must use exploratory data analysis and data visualization techniques. 

Then, you must perform data pre-processing operations such as ***Scaling*** and ***Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms. you are asked to perform ***Cluster Analysis*** based on the information you obtain during exploratory data analysis and data visualization processes. 

The purpose of clustering analysis is to cluster data with similar characteristics. You are asked to use the ***K-means*** algorithm to make cluster analysis. However, you must provide the K-means algorithm with information about the number of clusters it will make predictions. Also, the data you apply to the K-means algorithm must be scaled. In order to find the optimal number of clusters, you are asked to use the ***Elbow method***. Briefly, try to predict the set to which individuals are related by using K-means and evaluate the estimation results.

Once the data is ready to be applied to the model, you must ***split the data into train and test***. Then build a model to predict whether employees will churn or not. Train your models with your train set, test the success of your model with your test set. 

Try to make your predictions by using the algorithms ***Logistic Regression***, ***Random Forest Classifier***, ***XGBoost Classifier***, ***ANN***. You can use the related modules of the ***scikit-learn*** and ***Tensorflow-Keras*** library. You can use scikit-learn ***Classification Metrics*** module for accuracy calculation. You can use the ***Yellowbrick*** module for model selection and visualization.

In the final step, you will deploy your model using Streamlit tool.

# #Tasks

#### 1. Exploratory Data Analysis
- Importing Modules
- Loading Dataset
- Data Insigts

#### 2. Data Visualization
- Employees Left
- Determine Number of Projects
- Determine Time Spent in Company
- Subplots of Features

#### 3. Data Pre-Processing
- Scaling
- Label Encoding

#### 4. Cluster Analysis
- Find the optimal number of clusters (k) using the elbow method for for K-means.
- Determine the clusters by using K-Means then Evaluate predicted results.

#### 5. Model Building
- Split Data as Train and Test set
- Built Distance Based and Evaluate Model Performance and Predict Test Data
- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data
- Built XGBoost Classifier, Evaluate Model Performance and Predict Test Data
- Built ANN Classifier, Evaluate Model Performance and Predict Test Data

#### 6. Model Deployement

- Save and Export the Best Model
- Save and Export Variables
- Deploy best model via Streamlit

## 1. Exploratory Data Analysis

Exploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization.
"""

!pip install matplotlib==3.4

"""### Importing Modules"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
#importing packages needed

"""### Loading Dataset

Let's first load the required HR dataset using pandas's "read_csv" function.
"""

df = pd.read_csv('HR_Dataset.csv')

"""### Data Insights

In the given dataset, you have two types of employee one who stayed and another who left the company. So, you can divide data into two groups and compare their characteristics. Moreover, you can find the average of both the groups using groupby() and mean() function.
"""

df.head()

df.info()
#we have some int, float, and only two objects

df.describe().T

df.isnull().sum()
#checking for null values

df.isnull().sum()/df.shape[0] * 100
#there are no null values or missing values

df.columns

df.shape

df.duplicated().value_counts()

df.drop_duplicates(inplace = True)

df.shape

df.corr()

df.groupby(by='left').size()
#divide who left in to two groups
#here we see that 1991 employees left or potentially leaving and 10000 are still with the company

df.groupby(by='left').size().mean()
#the average of both the groups

df.groupby(by='left').size().describe()

df.groupby(by='left').corr()
#correlation between the two groups

plt.figure(figsize=(14,14))
sns.heatmap(df.groupby(by='left').corr(),annot=True,fmt='.0%')

"""## 2. Data Visualization

You can search for answers to the following questions using data visualization methods. Based on these responses, you can develop comments about the factors that cause churn.
- How does the promotion status affect employee churn?
- How does years of experience affect employee churn?
- How does workload affect employee churn?
- How does the salary level affect employee churn?

*Feel free to add more visualization

### Employees Left

Let's check how many employees were left?
Here, you can plot a bar graph using Matplotlib. The bar graph is suitable for showing discrete variable counts.
"""

salary = df.loc[df['left'] == 0, 'salary'].value_counts()
salary

salaries = df.loc[df['left'] == 1, 'salary'].value_counts()
salaries

sns.countplot(df['left'])

df.left.plot.hist()

"""### Number of Projects

Similarly, you can also plot a bar graph to count the number of employees deployed on how many projects?
"""

df.number_project.plot.hist(bins=30)

"""### Time Spent in Company

Similarly, you can also plot a bar graph to count the number of employees have based on how much experience?

"""

df.time_spend_company.plot.hist(bins=30)

"""### Subplots of Features

You can use the methods of the matplotlib.
"""

features = ['number_project','time_spend_company', 'Work_accident',
       'promotion_last_5years', 'Departments', 'salary']

list(enumerate(features))
#index number and name of feature

plt.figure(figsize=(15,20))
for i in enumerate(features):
  plt.subplot(3,3,i[0]+1)
  sns.countplot(i[1], hue='left',data=df)
  plt.xticks(rotation = 70)

#comeback and add more visualization

#most of the employees that have left are from the sales department then technical, and support. The least amount of churning comes from managment departments.
#How does workload affect employee churn?
#people with low amount of projects as well as a high number of projects are leaving
#How does the promotion status affect employee churn?
#there is a big coorelation between promotions and churning. A high number of people left when they were not promoted. Lower chances of them leaving if they are promoted
#How does the salary level affect employee churn?
#Low and medium salaries are churning out of the company with the lower the salary the higher they will leave.
#How does years of experience affect employee churn?
#We see that the people who have been there for over 7yrs are staying but the people between 3(being the highest churn) and 6yrs have the most people leaving. 3yrs are crucial for the company.

"""## 3. Data Pre-Processing

#### Scaling

Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Also distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.

Scaling Types:
- Normalization: Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.

- Standardization: Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.
"""

#normalization with MinMax scaling

from sklearn.preprocessing import MinMaxScaler

df.tail()

X = df.iloc[:,0:8]
X.head()

scalar = MinMaxScaler().fit(X)

print(scalar)

scalar.data_max_

scalar.data_min_

X.describe()

scalar.feature_range

df_scaled = scalar.transform(X)
df_scaled

# different forms of standardization

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
standardized_data = scaler.fit_transform(df.iloc[:,0:8])
print(standardized_data)

df_new = (df-df.mean())/df.std()
#For each feature we will compute its mean and standard deviation. Then we will subtract the mean from each observation and divide it by standard deviation to get the standardized values.

df_new.describe()

df_new.mean()

df_new.std()

df_new.head()

df_new.describe().round(2)

"""#### Encoding

Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column's value can be represented as low:0, medium:1, and high:2. This process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder.


"""

from sklearn import preprocessing

Label_encoder = preprocessing.LabelEncoder()

df['salary'].unique()

# Creating a map dict
mapping={'low':0, 'medium':1, 'high':2}

# Mapping
df['salary']= df['salary'].map(mapping)

df['salary'] = Label_encoder.fit_transform(df['salary'])
df['Departments'] = Label_encoder.fit_transform(df['Departments'])
#fit and transform salary and department columns into numerical columns

df['salary']

df.info()

"""## 4. Cluster Analysis

- Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.

    [Cluster Analysis](https://en.wikipedia.org/wiki/Cluster_analysis)

    [Cluster Analysis2](https://realpython.com/k-means-clustering-python/)

#### The Elbow Method

- "Elbow Method" can be used to find the optimum number of clusters in cluster analysis. The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k. If k increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.

    [The Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)

    [The Elbow Method2](https://medium.com/@mudgalvivek2911/machine-learning-clustering-elbow-method-4e8c2b404a5d)

    [KMeans](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)

Let's find out the groups of employees who left. You can observe that the most important factor for any employee to stay or leave is satisfaction and performance in the company. So let's bunch them in the group of people using cluster analysis.
"""

from sklearn.cluster import KMeans

wscc = []
for i in range(1,10): 
    kmeans = KMeans(n_clusters=i, init="k-means++",random_state=0)
    kmeans.fit(df_scaled)
    wscc.append(kmeans.inertia_)  

plt.plot(range(1,10),wscc,marker='8',c="purple")
plt.title("Elbow Method")
#The elbow in the graph is the five-cluster mark. This is the only place until which the graph is steeply declining while smoothing out afterward.

# Filter data
employee_left =  df[['satisfaction_level', 'last_evaluation']][df.left == 1]
# Create groups using K-means clustering.
kmeans = KMeans(n_clusters = 5, random_state = 0).fit(employee_left)

employee_left['cluster_label'] = kmeans.labels_
# Draw scatter plot
plt.scatter(employee_left['satisfaction_level'], employee_left['last_evaluation'], c=employee_left['cluster_label'],cmap='Accent')
plt.xlabel('Satisfaction Level')
plt.ylabel('Last Evaluation')
plt.title('3 Clusters of employees who left')
plt.show()

#The cream/orange shaded group are those with High Satisfaction and High Evaluation.
#The blue shaded group are those with Low Satisfaction and High Evaluation.
#The green shaded group are those with Moderate Satisfaction and moderate Evaluation.

"""## 5. Model Building

### Split Data as Train and Test Set
"""

y = df['left']

X = df[['satisfaction_level', 'last_evaluation', 'number_project',
       'average_montly_hours', 'time_spend_company', 'Work_accident',
              'promotion_last_5years','Departments', 'salary']]

df.columns = df.columns.str.strip()

from sklearn.model_selection import train_test_split

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)  # 70% training and 30% test

scaler = StandardScaler()
scaler.fit(X_train)
X_train = pd.DataFrame(scaler.transform(X_train), index = X_train.index, columns =X_train.columns)
X_test = pd.DataFrame(scaler.transform(X_test),index = X_test.index, columns =X_test.columns)

X.head()

scaler.fit(X_train)

X_train, X_test, y_train, y_test

"""### #Distance Based(?) Algorithm

#### Model Building
"""

#Confusion Matrix

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix,accuracy_score

#Logistic Regression Model
model = LogisticRegression()
model.fit(X_train,y_train)
pred = model.predict(X_test)

cm = confusion_matrix(y_test,pred)
cm

"""#### Prediction"""

matrix = classification_report(y_test,pred)
print('Classification Report: \n',matrix)

fig = plt.figure(figsize =(10,10))
ax = sns.heatmap(cm,annot =True, cmap="crest",fmt='.0f')
ax.set(xlabel="Predicted", ylabel="Truth")
ax.xaxis.tick_top()



"""#### Evaluating Model Performance and Tuning

- Confusion Matrix : You can use scikit-learn metrics module for accuracy calculation. A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.

    [Confusion Matrix](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/)

- Yellowbrick: Yellowbrick is a suite of visualization and diagnostic tools that will enable quicker model selection. It’s a Python package that combines scikit-learn and matplotlib. Some of the more popular visualization tools include model selection, feature visualization, classification and regression visualization

    [Yellowbrick](https://www.analyticsvidhya.com/blog/2018/05/yellowbrick-a-set-of-visualization-tools-to-accelerate-your-model-selection-process/)
"""

#Yellowbrick visulization



"""### #Random Forest Classifier

#### Model Building
"""

from sklearn.ensemble import RandomForestClassifier

rfcl = RandomForestClassifier().fit(X_train, y_train)

y_pred = rfcl.predict(X_test)

"""#### Evaluating Model Performance and Tuning"""

accuracy_score(y_test, y_pred)

rfcl.score(X_test,y_test)

"""#### Prediction"""

print(classification_report(y_test,y_pred))

#Conclusion On Accuracy of model 
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
# Model Precision
print("Precision:",metrics.precision_score(y_test, y_pred))
# Model Recall
print("Recall:",metrics.recall_score(y_test, y_pred))

"""### #XGBoost Classifier

#### Model Building
"""

import xgboost as xgb
boost_model = xgb.XGBClassifier().fit(X_train, y_train)

y_predict = boost_model.predict(X_test)

"""#### Evaluating Model Performance and Tuning"""

print(accuracy_score(y_test,y_predict))

boost_model.fit(X_train,y_train)
y_predict = boost_model.predict(X_test)
y_train_predict = boost_model.predict(X_train)
print('Train accuracy',accuracy_score(y_train,y_train_predict))
print('Test accuarcy',accuracy_score(y_test,y_predict))

"""#### Prediction"""

print(classification_report(y_test,y_predict))

"""### #ANN Classifier

#### Layer Architecture Building and Compiling
"""

from keras.models import Sequential
from keras.layers import Dense

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

classifier = Sequential()
# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 9))

# Adding the second hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Compiling the ANN | means applying SGD on the whole ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size = 15, epochs = 5)

score, acc = classifier.evaluate(X_train, y_train,
                            batch_size=10)

"""#### Evaluating Model Performance and Tunning"""

print('Train score:', score)
print('Train accuracy:', acc)
# Part 3 - Making predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)
print('Test score:', score)
print('Test accuracy:', acc)

"""#### Prediction"""

print(classification_report(y_test,y_pred))

#Conclusion On Accuracy of model 
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
# Model Precision
print("Precision:",metrics.precision_score(y_test, y_pred))
# Model Recall
print("Recall:",metrics.recall_score(y_test, y_pred))

"""## 6. Model Deployement

You cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployement helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied. Streamlit is a popular open source framework used by data scientists for model distribution.

Deployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.

Data science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values ​​can be extract from its predictions.

After doing the following operations in this notebook, jump to new .py file and create your web app with Streamlit.

### Save and Export the Model
"""

X.head(2)

y.head()

xgb_clf = xgb.XGBClassifier().fit(X, y)
rfcl = RandomForestClassifier().fit(X, y)

import pickle
pickle.dump(xgb_clf,open("XGBoost.pkl","wb"))
pickle.dump(rfcl,open("RF.pkl","wb"))

"""### Save and Export Variables

"""

XG_model=pickle.load(open("XGBoost.pkl","rb"))

XG_model.predict(X_test)

XG_model.predict_proba(X_test)#[0]

# One raw data
aa = df[:1].drop('left',axis=1)

# Check data
aa

# Create scalign object, only fitting with the whole dataframe, not transform
sc=StandardScaler().fit(df.drop('left', axis=1))

# Save it to pickle 
pickle.dump(sc,open("sc.pkl","wb"))

# Load scaling object from pickle
sc_model=pickle.load(open("sc.pkl","rb"))

# Now transform one line data, all values scaled correctly
sc_model.transform(aa)

# Try with fit_transform, see all values are zero
StandardScaler().fit_transform(aa)

"""___

<p style="text-align: center;"><img src="https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV" class="img-fluid" alt="CLRSWY"></p>

___
"""